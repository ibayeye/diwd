# -*- coding: utf-8 -*-

"""Support Vector Machine Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JSFgjIkvCRiQLhXD9wtEnBlnvdQaIPVb
"""

# 1. Importing Libraries


# Standard library
import re

# Third-party libraries
import numpy as np
import pandas as pd
import seaborn as sns
import gc
import matplotlib.pyplot as plt
import json

"""# 2. Reading Data

"""

#Load Dataset
gc.enable()
data_directory_path = 'csv/'

# Membaca dataset
df = pd.read_csv(data_directory_path + 'error_device.csv')

def load_dataset():
    # Info umum
    baris_kolom = {
        "jumlah_baris": df.shape[0],
        "jumlah_kolom": df.shape[1]
    }

    tipe_data = df.dtypes.astype(str).to_dict()
    missing_values = df.isnull().sum().to_dict()
    duplicate_count = int(df.duplicated().sum())
    deskripsi = df.describe().to_dict()
    sample_data = df.head().to_dict(orient='records')

    result = {
        "sample_data": sample_data,
        "baris_dan_kolom": baris_kolom,
        "tipe_data": tipe_data,
        "missing_values": missing_values,
        "duplikat": duplicate_count,
        "deskriptif": deskripsi
    }

    return result

"""# 3. Data Cleaning

"""

def get_cleaned_df():
    df_cleaned = df.drop_duplicates()
    df_cleaned = df_cleaned[df_cleaned["Timestamp"] != "#VALUE!"]
    df_cleaned["Timestamp"] = pd.to_datetime(df_cleaned["Timestamp"], errors="coerce")
    df_cleaned = df_cleaned.dropna(subset=["Timestamp"])

    def classify_error(message):
        if pd.isna(message):
            return 0
        message = str(message)
        if "Kuota habis" in message:
            return 0
        elif "ETIMEDOUT" in message or "EHOSTUNREACH" in message:
            return 1
        elif "ECONNREFUSED" in message or "ECONNRESET" in message:
            return 2
        else:
            return 0

    df_cleaned['Status'] = df_cleaned['Error Message'].apply(classify_error)
    return df_cleaned


def clean_dataset():
    try:
        df_cleaned = get_cleaned_df()
        summary = {
            "total_data_awal": df.shape[0],
            "total_data_setelah_cleaning": df_cleaned.shape[0],
            "missing_values": df_cleaned.isnull().sum().to_dict(),
            "distribusi_status": df_cleaned['Status'].value_counts().to_dict(),
            "contoh_data": df_cleaned.head(5).to_dict(orient='records')
        }
        return summary
    except Exception as e:
        return {"error": str(e)}

"""# 4. Exploratory Data Analysis (EDA)"""

def hourly_error_trend():
    try:
        df_cleaned = get_cleaned_df()
        df_cleaned["Hour"] = df_cleaned["Timestamp"].dt.hour
        error_by_hour = df_cleaned['Hour'].value_counts().sort_index()
        return error_by_hour.to_dict()
    except Exception as e:
        return {"error": str(e)}

def top_hourly_error_messages(top_n=5):
    try:
        df_cleaned = get_cleaned_df()
        df_cleaned["Hour"] = df_cleaned["Timestamp"].dt.hour

        top_errors_per_hour = (
            df_cleaned.groupby(["Hour", "Error Message"])
            .size()
            .reset_index(name="Count")
        )

        top_errors = (
            top_errors_per_hour
            .sort_values(['Hour', 'Count'], ascending=[True, False])
            .groupby('Hour')
            .head(top_n)
        )

        return top_errors.to_dict(orient='records')
    except Exception as e:
        return {"error": str(e)}
    
def daily_status_trend():
    try:
        df_cleaned = get_cleaned_df()
        df_cleaned["Date"] = df_cleaned["Timestamp"].dt.date
        status_labels = {0: 'Low', 1: 'Warning', 2: 'Critical'}

        daily_status_counts = (
            df_cleaned.groupby(["Date", "Status"])
            .size()
            .reset_index(name="Count")
        )

        last_7_days = sorted(df_cleaned["Date"].unique())[-7:]
        filtered = daily_status_counts[daily_status_counts["Date"].isin(last_7_days)]
        filtered["Status Label"] = filtered["Status"].map(status_labels)

        result = (
            filtered.groupby(["Date", "Status Label"])["Count"]
            .sum()
            .unstack(fill_value=0)
            .reset_index()
            .to_dict(orient='records')
        )

        return result
    except Exception as e:
        return {"error": str(e)}


def top_daily_error_messages(top_n=3):
    try:
        df_cleaned = get_cleaned_df()
        df_cleaned["Date"] = df_cleaned["Timestamp"].dt.date

        daily_error_counts = (
            df_cleaned.groupby(["Date", "Error Message"])
            .size()
            .reset_index(name="Count")
        )

        last_7_days = sorted(df_cleaned["Date"].unique())[-7:]
        filtered = daily_error_counts[daily_error_counts["Date"].isin(last_7_days)]

        top_daily_errors = (
            filtered.sort_values(["Date", "Count"], ascending=[True, False])
            .groupby("Date")
            .head(top_n)
        )

        return top_daily_errors.to_dict(orient='records')
    except Exception as e:
        return {"error": str(e)}

def weekly_status_trend():
    try:
        df_cleaned = get_cleaned_df()
        df_cleaned["Week"] = df_cleaned["Timestamp"].dt.to_period("W").apply(lambda r: r.start_time.date())
        status_labels = {0: 'Low', 1: 'Warning', 2: 'Critical'}

        weekly_status_counts = (
            df_cleaned.groupby(["Week", "Status"])
            .size()
            .reset_index(name="Count")
        )

        last_5_weeks = sorted(df_cleaned["Week"].unique())[-5:]
        filtered = weekly_status_counts[weekly_status_counts["Week"].isin(last_5_weeks)]
        filtered["Status Label"] = filtered["Status"].map(status_labels)

        result = (
            filtered.groupby(["Week", "Status Label"])["Count"]
            .sum()
            .unstack(fill_value=0)
            .reset_index()
            .to_dict(orient='records')
        )

        return result
    except Exception as e:
        return {"error": str(e)}


def top_weekly_error_messages(top_n=3):
    try:
        df_cleaned = get_cleaned_df()
        df_cleaned["Week"] = df_cleaned["Timestamp"].dt.to_period("W").apply(lambda r: r.start_time.date())

        weekly_error_counts = (
            df_cleaned.groupby(["Week", "Error Message"])
            .size()
            .reset_index(name="Count")
        )

        last_5_weeks = sorted(df_cleaned["Week"].unique())[-5:]
        filtered = weekly_error_counts[weekly_error_counts["Week"].isin(last_5_weeks)]

        top_weekly_errors = (
            filtered.sort_values(["Week", "Count"], ascending=[True, False])
            .groupby("Week")
            .head(top_n)
        )

        return top_weekly_errors.to_dict(orient='records')
    except Exception as e:
        return {"error": str(e)}

def monthly_status_trend():
    try:
        df_cleaned = get_cleaned_df()
        df_cleaned["Month"] = df_cleaned["Timestamp"].dt.to_period("M").astype(str)
        monthly_trend = (
            df_cleaned.groupby(["Month", "Status"])
            .size()
            .reset_index(name="Count")
        )
        status_labels = {0: "Low", 1: "Warning", 2: "Critical"}
        monthly_trend["Status Label"] = monthly_trend["Status"].map(status_labels)
        return monthly_trend.to_dict(orient='records')
    except Exception as e:
        return {"error": str(e)}

def top_monthly_error_messages(top_n=3):
    try:
        df_cleaned = get_cleaned_df()
        df_cleaned["Month"] = df_cleaned["Timestamp"].dt.to_period("M").astype(str)

        monthly_error_msg = (
            df_cleaned.groupby(["Month", "Error Message"])
            .size()
            .reset_index(name="Count")
        )

        top_monthly = (
            monthly_error_msg
            .sort_values(["Month", "Count"], ascending=[True, False])
            .groupby("Month")
            .head(top_n)
        )
        return top_monthly.to_dict(orient="records")
    except Exception as e:
        return {"error": str(e)}

def error_status_distribution():
    try:
        df_cleaned = get_cleaned_df()
        status_labels = {0: "Low", 1: "Warning", 2: "Critical"}
        df_cleaned['Status Label'] = df_cleaned['Status'].map(status_labels)
        distribution = df_cleaned['Status Label'].value_counts().to_dict()
        return distribution
    except Exception as e:
        return {"error": str(e)}

def top_error_messages_per_status(top_n=5):
    try:
        df_cleaned = get_cleaned_df()
        status_labels = {0: "Low", 1: "Warning", 2: "Critical"}
        df_cleaned['Status Label'] = df_cleaned['Status'].map(status_labels)

        top_errors_by_status = (
            df_cleaned.groupby(['Status Label', 'Error Message'])
            .size()
            .reset_index(name='Count')
            .sort_values(['Status Label', 'Count'], ascending=[True, False])
        )

        top_errors_filtered = (
            top_errors_by_status.groupby('Status Label')
            .head(top_n)
            .reset_index(drop=True)
        )

        return top_errors_filtered.to_dict(orient='records')
    except Exception as e:
        return {"error": str(e)}
